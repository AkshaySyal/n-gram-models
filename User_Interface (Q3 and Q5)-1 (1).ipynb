{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1 and Q2 are in LLM_HW1.ipynb. Q3 and Q4 are in Report.pdf. Q5 in User_Interface.ipynb"
      ],
      "metadata": {
        "id": "8rgzGkRd0MjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Bonus: Create a simple user interface where users can enter some preffix and get a completion of words up to a specified length."
      ],
      "metadata": {
        "id": "I28tnhnLdZZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5VHD38Olpe_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3E6sLHDmMdM",
        "outputId": "b645f77f-210f-46cf-cce2-4b3e0cb2932d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# Tokenizer\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NliyMMCp9KF1",
        "outputId": "51b9e538-67e8-4654-92ca-bc886715c2f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMNSbiBfPzT3",
        "outputId": "337581ff-6cf6-4a50-e6a7-c0b8d8e9b1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "nltk.download('reuters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5XiGBz48i_0"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvaH_UqWP4WR"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import reuters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSZuZ4xjmOQv"
      },
      "outputs": [],
      "source": [
        "# Idea is to prevent recalculation of preprocessing and n-gram freq\n",
        "# each time I change sentence\n",
        "# or decide to change dataset, n or smoothing technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7zKc4vYppw-"
      },
      "outputs": [],
      "source": [
        "class NgramModel:\n",
        "  def __init__(self, dataset, n=3, smoothing_technique='None'):\n",
        "    self.n = n\n",
        "    self.smoothing_technique = smoothing_technique\n",
        "    self.preprocessed_dataset = NgramModel.preprocessing_pipeline(dataset)\n",
        "    self.tokens = self.generate_tokens()\n",
        "    self.ngram_frequency_dict = self.generate_ngram_frequency_dict(n)\n",
        "    self.n_minus_1_gram_frequency_dict = self.generate_ngram_frequency_dict(n-1)\n",
        "    self.unigram_frequency_dict = self.generate_ngram_frequency_dict(1)\n",
        "    self.unique_tokens = self.generate_unique_tokens()\n",
        "\n",
        "    if smoothing_technique=='linear_interpolation':\n",
        "      self.n = 3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def roman_to_int(s):\n",
        "    roman_numerals = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n",
        "\n",
        "    result = 0\n",
        "    prev_value = 0\n",
        "\n",
        "    for char in reversed(s):\n",
        "        value = roman_numerals[char]\n",
        "        if value < prev_value:\n",
        "            result -= value\n",
        "        else:\n",
        "            result += value\n",
        "        prev_value = value\n",
        "\n",
        "    return result\n",
        "\n",
        "  @staticmethod\n",
        "  def extract_and_convert_roman(input_string):\n",
        "    # Find all instances of \"VOLUME\" or \"CHAPTER\" followed by a space and then capture the Roman numeral\n",
        "    matches = re.finditer(r'(VOLUME|CHAPTER)\\s([IVXLCDM]+)', input_string)\n",
        "\n",
        "    for match in matches:\n",
        "      keyword, roman_numeral = match.groups()\n",
        "\n",
        "      # Convert Roman numeral to integer\n",
        "      result = NgramModel.roman_to_int(roman_numeral)\n",
        "\n",
        "      input_string = input_string.replace(f\"{keyword} {roman_numeral}\", f\"{keyword} {result}\")\n",
        "\n",
        "    return input_string\n",
        "\n",
        "  @staticmethod\n",
        "  def remove_non_alphanumeric(input_string):\n",
        "    # Use regular expression to replace non-alphanumeric characters with an empty string\n",
        "    result_string = re.sub(r'[^a-zA-Z0-9 \\n]', '', input_string)\n",
        "    result_string = re.sub(r'\\n+', ' ', result_string)\n",
        "    return result_string\n",
        "\n",
        "  @staticmethod\n",
        "  def lowercase_string(input_string):\n",
        "      # Use lower() method to convert the string to lowercase\n",
        "      result_string = input_string.lower()\n",
        "      return result_string\n",
        "\n",
        "  @staticmethod\n",
        "  def preprocessing_pipeline(input_text):\n",
        "      converted_roman_text = NgramModel.extract_and_convert_roman(input_text)\n",
        "      cleaned_text = NgramModel.remove_non_alphanumeric(converted_roman_text)\n",
        "      lower_cased_text = NgramModel.lowercase_string(cleaned_text)\n",
        "      return lower_cased_text\n",
        "\n",
        "  @staticmethod\n",
        "  def generate_gram(seq,n):\n",
        "    seq_list = seq.split(' ')\n",
        "    return ' '.join(seq_list[-n+1:])\n",
        "\n",
        "  def generate_tokens(self):\n",
        "    return nltk.word_tokenize(self.preprocessed_dataset)\n",
        "\n",
        "  def generate_unique_tokens(self):\n",
        "    return list(set(self.tokens))\n",
        "\n",
        "  def generate_ngram_frequency_dict(self,n):\n",
        "    n_grams = ngrams(self.tokens,n=n)\n",
        "    n_grams_lis = list(n_grams)\n",
        "    ngram_frequency = Counter(n_grams_lis)\n",
        "    ngram_frequency_dict = dict(ngram_frequency)\n",
        "\n",
        "    return ngram_frequency_dict\n",
        "\n",
        "  def probability(self,word,gram):\n",
        "    if self.smoothing_technique == 'laplace':\n",
        "      return self.laplace_smoothing_probability(word,gram)\n",
        "\n",
        "    elif self.smoothing_technique == 'linear_interpolation':\n",
        "      return self.linear_interpolation_probability(word,gram)\n",
        "\n",
        "    else:\n",
        "      return self.no_smoothing_probability(word,gram)\n",
        "\n",
        "  def laplace_smoothing_probability(self,word,gram):\n",
        "    prob = 0\n",
        "    n_minus_1_gram_list = gram.split(' ')\n",
        "\n",
        "    n_gram_list = []\n",
        "    n_gram_list.extend(n_minus_1_gram_list)\n",
        "    n_gram_list.extend([word])\n",
        "\n",
        "    if tuple(n_gram_list) in self.ngram_frequency_dict:\n",
        "      N = self.ngram_frequency_dict[tuple(n_gram_list)]+1\n",
        "    else:\n",
        "      N = 1\n",
        "\n",
        "    if tuple(n_minus_1_gram_list) in self.n_minus_1_gram_frequency_dict:\n",
        "      D = self.n_minus_1_gram_frequency_dict[tuple(n_minus_1_gram_list)]+len(self.unique_tokens)\n",
        "    else:\n",
        "      D = len(self.unique_tokens)\n",
        "\n",
        "    prob = N/D\n",
        "\n",
        "    return prob\n",
        "\n",
        "  # gram param is a bigram\n",
        "  def linear_interpolation_probability(self,word,gram):\n",
        "    # word = mat, gram = on a\n",
        "    prob1,prob2,prob3,prob = 0,0,0,0\n",
        "    n_minus_1_gram_list = gram.split(' ') # [on,a]\n",
        "    n_gram_list = []\n",
        "    n_gram_list.extend(n_minus_1_gram_list)\n",
        "    n_gram_list.extend([word]) # [on,a,mat]\n",
        "\n",
        "    try:\n",
        "      prob1 = self.ngram_frequency_dict[tuple(n_gram_list)] / self.n_minus_1_gram_frequency_dict[tuple(n_minus_1_gram_list)]\n",
        "    except Exception as e:\n",
        "      pass\n",
        "\n",
        "\n",
        "    n_minus_1_gram_list.pop(0) # [a]\n",
        "    n_gram_list.pop(0) # [a,mat]\n",
        "\n",
        "    try:\n",
        "      prob2 = self.n_minus_1_gram_frequency_dict[tuple(n_gram_list)] / self.unigram_frequency_dict[tuple(n_minus_1_gram_list)]\n",
        "    except Exception as e:\n",
        "      pass\n",
        "\n",
        "\n",
        "    n_gram_list.pop(0) # [mat]\n",
        "\n",
        "    try:\n",
        "      prob3 = self.unigram_frequency_dict[tuple(n_gram_list)] / len(self.unique_tokens)\n",
        "    except Exception as e:\n",
        "      pass\n",
        "\n",
        "    prob = 0.6*prob1 + 0.28*prob2 + 0.12*prob3\n",
        "    return prob\n",
        "\n",
        "  def no_smoothing_probability(self,word,gram):\n",
        "    prob = 0\n",
        "    n_minus_1_gram_list = gram.split(' ')\n",
        "    n_gram_list = []\n",
        "    n_gram_list.extend(n_minus_1_gram_list)\n",
        "    n_gram_list.extend([word])\n",
        "\n",
        "    try:\n",
        "      prob = self.ngram_frequency_dict[tuple(n_gram_list)] / self.n_minus_1_gram_frequency_dict[tuple(n_minus_1_gram_list)]\n",
        "    except Exception as e:\n",
        "      pass\n",
        "\n",
        "    return prob\n",
        "\n",
        "  def nextWord(self,seq):\n",
        "\n",
        "    max_prob,nxt_word = 0,''\n",
        "    for token in self.unique_tokens:\n",
        "      token_prob = self.probability(token,NgramModel.generate_gram(seq,self.n))\n",
        "      if token_prob>max_prob:\n",
        "        max_prob = token_prob\n",
        "        nxt_word = token\n",
        "\n",
        "    # if our seq has a word which is not in vocab then\n",
        "    # prob of all tokens in vocab is equal to 1/len(vocab)\n",
        "    # hence random token from vocab should be returned\n",
        "\n",
        "    if max_prob == 1/len(self.unique_tokens) and self.smoothing_technique=='laplace':\n",
        "      nxt_word = random.choice(self.unique_tokens)\n",
        "    return nxt_word\n",
        "\n",
        "\n",
        "  def generateSentence(self,sentence_length,prefix):\n",
        "    generated_sentence = prefix\n",
        "\n",
        "    for i in range(sentence_length):\n",
        "      generated_sentence += ' ' + self.nextWord(generated_sentence)\n",
        "\n",
        "    return generated_sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVvqHFDfPKwN"
      },
      "source": [
        "My Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqRNkLlu6M54"
      },
      "outputs": [],
      "source": [
        "# Dataset: Gutenberg, n=2, smoothing=None\n",
        "model_guten_2_no_smoothing = NgramModel(dataset=gutenberg.raw(), n=2, smoothing_technique='None')\n",
        "\n",
        "# Dataset: Gutenberg, n=3, smoothing=None\n",
        "model_guten_3_no_smoothing = NgramModel(dataset=gutenberg.raw(), n=3, smoothing_technique='None')\n",
        "\n",
        "# Dataset: Gutenberg, n=4, smoothing=None\n",
        "model_guten_4_no_smoothing = NgramModel(dataset=gutenberg.raw(), n=4, smoothing_technique='None')\n",
        "\n",
        "# Dataset: Gutenberg, n=5, smoothing=None\n",
        "model_guten_5_no_smoothing = NgramModel(dataset=gutenberg.raw(), n=5, smoothing_technique='None')\n",
        "\n",
        "# Dataset: Gutenberg, n=4, smoothing=laplace\n",
        "model_guten_4_laplace = NgramModel(dataset=gutenberg.raw(), n=4, smoothing_technique='laplace')\n",
        "\n",
        "# Dataset: Gutenberg, n=3 by default, smoothing=linear_interpolation\n",
        "model_guten_3_linear_interpolation = NgramModel(dataset=gutenberg.raw(), n=3, smoothing_technique='linear_interpolation')\n",
        "\n",
        "# Dataset: Reuters, n=4, smoothing=None\n",
        "model_reuters_4_no_smoothing = NgramModel(dataset=reuters.raw(), n=4, smoothing_technique='None')\n",
        "\n",
        "# Dataset: Reuters, n=2, smoothing=laplace\n",
        "model_reuters_2_laplace = NgramModel(dataset=reuters.raw(), n=2, smoothing_technique='laplace')\n",
        "\n",
        "# Dataset: Reuters, n=3, smoothing=linear_interpolation\n",
        "model_reuters_3_linear_interpolation = NgramModel(dataset=reuters.raw(), n=3, smoothing_technique='linear_interpolation')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interface"
      ],
      "metadata": {
        "id": "M6BZSv6LTaXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input('Enter prefix: ')\n",
        "length = input('Enter number of words to generate: ')\n",
        "length = int(length)\n",
        "\n",
        "print(f'model_guten_2_no_smoothing: {model_guten_2_no_smoothing.generateSentence(length,prompt)}')\n",
        "print(f'model_guten_3_no_smoothing: {model_guten_3_no_smoothing.generateSentence(length,prompt)}')\n",
        "print(f'model_guten_4_no_smoothing: {model_guten_4_no_smoothing.generateSentence(length,prompt)}')\n",
        "print(f'model_guten_5_no_smoothing: {model_guten_5_no_smoothing.generateSentence(length,prompt)}')\n",
        "print(f'model_guten_4_laplace: {model_guten_4_laplace.generateSentence(length,prompt)}')\n",
        "print(f'model_guten_3_linear_interpolation: {model_guten_3_linear_interpolation.generateSentence(length,prompt)}')\n",
        "print(f'model_reuters_4_no_smoothing: {model_reuters_4_no_smoothing.generateSentence(length,prompt)}')\n",
        "print(f'model_reuters_2_laplace: {model_reuters_2_laplace.generateSentence(length,prompt)}')\n",
        "print(f'model_reuters_3_linear_interpolation: {model_reuters_3_linear_interpolation.generateSentence(length,prompt)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njWgvB39TYrP",
        "outputId": "b29c2648-5c22-418e-a616-c7219b9d2e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter prefix: i am going to\n",
            "Enter number of words to generate: 5\n",
            "model_guten_2_no_smoothing: i am going to the lord and the lord\n",
            "model_guten_3_no_smoothing: i am going to be a great deal of\n",
            "model_guten_4_no_smoothing: i am going to tell you that you will\n",
            "model_guten_5_no_smoothing: i am going to tell you to any son\n",
            "model_guten_4_laplace: i am going to tell you that you will\n",
            "model_guten_3_linear_interpolation: i am going to the the lobsters and the\n",
            "model_reuters_4_no_smoothing: i am going to     \n",
            "model_reuters_2_laplace: i am going to the company said the company\n",
            "model_reuters_3_linear_interpolation: i am going to the the the the the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ie8SPtTETt0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}